# **Politique du numérique à l’ère des LLM**

## **Automatisation du tertiaire, infrastructures cloud et recomposition de la souveraineté économique**

### Introduction

L’émergence des grands modèles de langage (LLM) marque un tournant majeur dans l’histoire récente du numérique. Plus qu’un progrès technique ponctuel, elle introduit un changement de régime économique fondé sur la réduction massive du coût cognitif : écrire du code, intégrer des systèmes complexes, exploiter des infrastructures numériques ou automatiser des processus tertiaires devient plus rapide, moins coûteux et accessible à un nombre élargi d’acteurs. Cette évolution affecte simultanément la productivité, l’emploi qualifié, l’organisation des filières numériques et les conditions concrètes de l’autonomie économique.

Pour la France, dont l’économie repose largement sur un tertiaire à forte valeur ajoutée, cette transformation constitue à la fois un facteur de fragilisation et une opportunité stratégique. Les secteurs historiquement structurants — banque, assurance, conseil, services informatiques, fonctions de conformité — voient une part croissante de leurs activités automatisables. Dans le même temps, les LLM rendent de nouveau envisageable une politique du numérique orientée vers l’industrialisation, l’intégration et la maîtrise des usages critiques, sans nécessiter la reproduction du modèle d’hyperscale porté par les grandes plateformes mondiales.

L’autonomie numérique ne se joue plus uniquement dans la conception des modèles, mais dans la capacité à opérer des clouds robustes, sécurisés, réversibles et adaptés aux secteurs régulés, sans dépendre structurellement d’infrastructures extra-européennes.
Par ailleurs, ces dynamiques dépassent largement le cadre européen. Les trajectoires observables en Afrique, où les LLM facilitent des stratégies de leapfrogging et l’émergence possible de clouds régionaux, illustrent la portée globale de ces transformations. Elles contribuent à redéfinir les rapports d’influence fondés sur l’exportation de services tertiaires et interrogent les modalités futures de la coopération économique et numérique.

Ce rapport propose une analyse structurée de ces mutations. Il examine successivement la redéfinition des conditions d’une politique du numérique, l’impact des LLM sur le tertiaire, le rôle central du cloud dans l’autonomie numérique, les implications géoéconomiques associées et les instruments permettant de transformer ces constats en trajectoire opérationnelle. L’objectif n’est pas de formuler un programme politique, mais d’apporter un cadre d’analyse économique permettant de comprendre les effets systémiques de l’IA générative sur la productivité, les infrastructures et l’autonomie numérique.

## PARTIE I — Accessibilité : pourquoi une politique du numérique redevient possible pour la France

### I.1. Le changement de régime technologique : démocratisation partielle de la puissance

L’irruption des grands modèles de langage (LLM) marque un changement de régime technologique comparable, par ses effets économiques, à l’informatisation des années 1980 ou à la généralisation d’Internet dans les années 2000. La nouveauté ne réside pas uniquement dans la performance des modèles, mais dans la **baisse drastique du coût cognitif du développement**. Une part croissante des tâches traditionnellement réservées à des ingénieurs ou à des experts — écriture de code, tests, documentation, support, configuration d’infrastructures — devient automatisable ou semi-automatisable.

Cette dynamique produit trois effets macroéconomiques majeurs. D’abord, une **accélération des cycles de R&D** : le temps séparant l’idée du produit fonctionnel se réduit fortement. Ensuite, une **compression des cycles produits**, avec des itérations plus fréquentes et des coûts d’entrée plus faibles. Enfin, une **réduction du seuil d’accès à des capacités avancées**, permettant à des acteurs de taille moyenne de développer des services complexes auparavant réservés aux grandes plateformes.

Toutefois, cette démocratisation reste **partielle**. L’accès à un modèle, qu’il soit open source ou fourni via API, ne se confond pas avec la capacité à en faire un actif productif. L’avantage compétitif se déplace vers la **capacité d’industrialisation** : qualité et gouvernance des données, infrastructures de calcul et de stockage, sécurité des chaînes logicielles, intégration dans des processus métiers réels, et exploitation à l’échelle. Autrement dit, la puissance ne disparaît pas ; elle change de support.

### I.2. Souveraineté numérique : de l’idéologie au calcul économique

Dans ce nouveau contexte, la souveraineté numérique cesse d’être un slogan ou une posture défensive pour devenir un **problème de calcul économique**. Une souveraineté dite « efficace » ne se définit plus par l’autarcie technologique, mais par la **maîtrise des usages critiques** : systèmes d’information de l’État, données de santé, infrastructures financières, chaînes industrielles stratégiques. Elle se mesure également à la **résilience** face aux ruptures technologiques, géopolitiques ou réglementaires.

L’enjeu central devient alors le **coût total de dépendance**. Celui-ci agrège des dimensions longtemps sous-estimées : dépendance aux données et à leur hébergement, coûts de conformité imposés par des normes extraterritoriales, verrouillage technologique (lock-in) lié aux plateformes, exposition juridique et stratégique à des décisions étrangères. Dans de nombreux cas, ces coûts différés excèdent les gains immédiats de productivité offerts par les solutions dominantes.

Ainsi comprise, la souveraineté numérique ne vise pas à exclure les technologies étrangères, mais à **arbitrer rationnellement** entre gains à court terme et dépendances structurelles à long terme. Les LLM, en abaissant les coûts de développement et d’intégration, rendent pour la première fois cet arbitrage économiquement crédible pour des acteurs nationaux et européens.

### I.3. La France : atouts structurels et angles morts

La France dispose d’atouts significatifs pour transformer cette fenêtre technologique en politique publique cohérente. Elle conserve une **base industrielle diversifiée**, un **secteur public de grande taille** capable de jouer un rôle de client structurant, un appareil réglementaire reconnu, ainsi que des **acteurs télécoms et cloud européens** disposant d’infrastructures réelles. À cela s’ajoute un écosystème de compétences en mathématiques, en ingénierie et en cybersécurité, historiquement solide.

Ces atouts coexistent cependant avec des **angles morts persistants**. Le premier est l’absence d’acteurs d’hyperscale capables de rivaliser frontalement avec les grandes plateformes américaines sur le terrain du volume et du coût unitaire du calcul. Le second est une **dépendance critique** aux chaînes d’approvisionnement en GPU et aux services cloud extra-européens pour l’entraînement et l’exploitation à grande échelle des modèles. Enfin, l’offre française et européenne reste **fragmentée**, tant sur le plan industriel que commercial, ce qui complique l’émergence de solutions intégrées et lisibles à l’international.

C’est précisément dans la tension entre ces atouts et ces limites que se situe l’enjeu de la politique du numérique : non pas reproduire le modèle des hyperscalers, mais construire une capacité d’industrialisation adaptée aux usages critiques, économiquement soutenable et stratégiquement maîtrisée.


## **PARTIE II — Le tertiaire français face aux LLM : productivité, emploi, rente et repositionnement**

### **II.1. Le tertiaire comme « moteur historique » : ce qui est remis en cause**

Depuis plusieurs décennies, l’économie française repose largement sur un **tertiaire à forte valeur ajoutée**, structuré autour de secteurs tels que la banque et l’assurance, le conseil, l’audit, le juridique ou encore les services informatiques. La valeur économique de ces activités ne provenait pas principalement du capital physique, mais d’une **expertise procédurale** (maîtrise de règles, de normes, de méthodes) et d’une **intermédiation cognitive** : analyser, interpréter, qualifier, décider pour le compte d’autrui.

Ce modèle a permis à la France de compenser partiellement l’érosion de sa base industrielle en exportant des services, du savoir-faire réglementaire et de la complexité organisée. Or, c’est précisément ce socle — la capacité à transformer la complexité en rente — que les LLM viennent fragiliser. En automatisant une part croissante des tâches cognitives standardisées, ils réduisent la rareté qui justifiait historiquement la valeur de ces intermédiaires.

La remise en cause n’est donc pas marginale : elle touche le **cœur économique du tertiaire français**, là où se concentraient marges, emplois qualifiés et influence internationale.

### **II.2. Ce qui s’automatise réellement… et ce qui résiste**

Contrairement à une vision caricaturale, les LLM n’automatisent pas « le tertiaire » dans son ensemble, mais opèrent une **segmentation nette** entre tâches industrialisables et fonctions à forte dimension stratégique ou relationnelle.

Sont d’ores et déjà largement automatisables, ou en passe de l’être : le back-office administratif et financier, le support client de premier niveau, la conformité réglementaire de niveau 1 (KYC, contrôles standards), le reporting récurrent, la maintenance applicative courante, ainsi qu’une partie significative du développement logiciel standard. Dans ces domaines, la valeur ajoutée humaine décroît rapidement, et la concurrence se fait principalement sur les coûts et l’intégration.

À l’inverse, certaines fonctions résistent structurellement à l’automatisation. Il s’agit notamment de l’arbitrage dans des situations ambiguës, de la relation client complexe ou sensible, de la gouvernance et de la prise de décision stratégique, de la conception de produits ou de systèmes, de la négociation, ainsi que de la gestion des risques rares ou systémiques. Ces activités mobilisent du jugement, de la responsabilité et une compréhension contextuelle profonde que les LLM ne font qu’assister, sans pouvoir s’y substituer.

L’enjeu pour le tertiaire français n’est donc pas de refuser l’automatisation, mais de **se repositionner sur ces zones de résistance**, là où la valeur reste durable.

### **II.3. Effets macroéconomiques : choc de productivité et choc de structure**

À l’échelle macroéconomique, l’irruption des LLM produit un **double choc**. D’une part, un choc de productivité : les services industrialisables voient leurs coûts unitaires chuter, ce qui améliore l’efficacité globale. D’autre part, un choc de structure : les **marges des activités tertiaires standardisées se compressent fortement**, mettant sous pression des modèles économiques entiers.

Cette dynamique fait émerger un risque majeur : la **désintermédiation** des acteurs français au profit de grandes plateformes capables d’industrialiser à l’échelle mondiale ces services automatisés. Dans ce scénario, la France consommerait des services à haute productivité sans en capter la valeur, reproduisant dans le tertiaire la dépendance observée dans d’autres secteurs.

Mais cette transformation ouvre également une opportunité stratégique. La baisse des coûts cognitifs permet une **réinternalisation** de fonctions auparavant externalisées, notamment au sein de l’industrie et des entreprises de taille intermédiaire. Elle favorise aussi une **montée en gamme** du tertiaire vers des activités d’architecture, d’expertise de haut niveau, de cybersécurité et de gouvernance des systèmes complexes.

### **II.4. Politique publique : amortir le choc, capter le gain**

Face à cette reconfiguration, la politique publique a un rôle déterminant pour éviter que le choc ne se traduise uniquement par des destructions d’emplois et une perte de valeur nationale. Le premier levier est la **requalification** : développer massivement les compétences liées à l’intégration de l’IA, à la cybersécurité, à la gouvernance des données et aux opérations cloud, là où la demande structurelle va croître.

Le second levier réside dans les **instruments économiques**. Les achats publics peuvent jouer un rôle de client ancre pour orienter le marché vers des solutions souveraines et intégrées. Des crédits ciblés peuvent soutenir la transformation des acteurs tertiaires existants. La normalisation — en matière de sécurité, de réversibilité et d’auditabilité des systèmes — permet de structurer un marché favorable aux acteurs nationaux. Enfin, la protection et la maîtrise des secteurs critiques (finance, santé, État, énergie) constituent un socle indispensable pour capter durablement les gains de productivité générés par les LLM.

Ainsi, loin d’annoncer la fin du tertiaire, les LLM imposent une **mutation profonde** : d’un tertiaire fondé sur la complexité procédurale vers un tertiaire de conception, de gouvernance et de maîtrise des systèmes automatisés.

## PARTIE III — Cloud : le nouveau centre de gravité de l’autonomie numérique

### (LLM → intégration + développement → indépendance)

### III.1. Le développement de solutions cloud complexes comme infrastructure de souveraineté devient accessible

Pendant plus d’une décennie, le cloud a été perçu comme un domaine réservé à quelques acteurs capables d’absorber des coûts de développement, d’exploitation et d’intégration extrêmement élevés. La complexité technique — orchestration, sécurité, observabilité, automatisation, support — constituait une barrière à l’entrée majeure. L’arrivée des LLM modifie profondément cet équilibre.

En automatisant une partie significative du **développement logiciel**, de la configuration des infrastructures, de la gestion des incidents et de la documentation, les LLM abaissent le coût cognitif et organisationnel de la construction de solutions cloud complexes. Il devient possible, pour un nombre beaucoup plus large d’acteurs du numérique, de concevoir et d’exploiter des plateformes cloud fonctionnelles, sécurisées et adaptées à des usages professionnels.

Cette démocratisation ne signifie pas la disparition des écarts de puissance, mais un **élargissement du champ des acteurs capables d’opérer des infrastructures souveraines**. Le cloud cesse progressivement d’être un monopole industriel pour devenir une infrastructure modulable, reproductible et intégrable, à condition de maîtriser l’ingénierie d’ensemble. À ce titre, il s’impose comme une **infrastructure de souveraineté**, au même titre que l’énergie ou les télécommunications.

### III.2. La politique numérique française comme facteur de développement

Dans ce nouveau contexte, la politique numérique de la France peut redevenir un **levier de développement économique**, et non un simple cadre de régulation. La baisse des coûts d’intégration induite par les LLM rend crédible une stratégie visant à structurer un écosystème national et européen du cloud, orienté vers les usages critiques et les besoins réels des administrations et des entreprises.

Plutôt que de chercher à concurrencer frontalement les hyperscalers sur le terrain de l’échelle globale, la France dispose d’une opportunité pour favoriser des **solutions cloud spécialisées**, intégrées aux contraintes réglementaires, sectorielles et territoriales. La politique publique peut jouer un rôle catalyseur en orientant la demande, en stabilisant les standards et en réduisant le risque pour les acteurs industriels.

Ainsi conçue, la politique numérique ne se limite plus à la protection, mais devient un **outil d’industrialisation** : elle favorise l’émergence de chaînes de valeur locales, soutient l’investissement dans les infrastructures et accélère la diffusion de solutions cloud adaptées aux secteurs régulés.

### III.3. France : fenêtre stratégique pour OVHcloud, Orange et les alliances industrielles

La France dispose, dans ce contexte, d’une fenêtre stratégique pour des acteurs tels qu’OVHcloud et Orange, ainsi que pour des alliances industrielles associant télécoms, fournisseurs cloud, intégrateurs et acteurs de la cybersécurité. Leur positionnement naturel repose sur un modèle de **cloud de proximité**, conçu pour répondre aux exigences des secteurs régulés et des infrastructures critiques.

Ce modèle combine plusieurs dimensions clés : l’hybridation entre systèmes sur site et cloud, garantissant la continuité opérationnelle ; la conformité réglementaire et la souveraineté juridique des données ; la réversibilité des architectures afin de limiter les situations de verrouillage ; et un haut niveau de sécurité intégré dès la conception. L’intégration des réseaux télécoms, des capacités cloud et des briques d’IA constitue un facteur de différenciation majeur, difficilement réplicable par des acteurs purement plateformes.

Dans cette configuration, la valeur ne réside pas dans la taille absolue du cloud, mais dans sa **capacité d’intégration**, sa proximité avec les utilisateurs et sa maîtrise des contraintes critiques.

### III.4. Les limites structurelles : GPU, chaînes d’approvisionnement et standards

Malgré ces opportunités, certaines limites demeurent structurelles. La dépendance aux semi-conducteurs avancés, en particulier aux GPU, constitue un point de fragilité majeur, tout comme la dépendance énergétique associée aux infrastructures de calcul intensif. Ces contraintes rappellent que l’autonomie numérique ne peut être pensée indépendamment des politiques industrielles et énergétiques.

Par ailleurs, les standards techniques et logiciels — souvent définis à l’échelle internationale — conditionnent fortement l’interopérabilité et la compétitivité des solutions cloud. L’enjeu n’est pas de s’en abstraire, mais de **peser dans leur définition** et de garantir la compatibilité des solutions nationales avec les écosystèmes globaux.

Face à ces limites, des stratégies complémentaires s’imposent : mutualisation européenne des capacités de calcul, partenariats industriels ciblés, politiques de procurement souverain pour sécuriser les approvisionnements critiques. Ces leviers conditionnent la capacité à transformer la fenêtre ouverte par les LLM en autonomie durable plutôt qu’en dépendance déplacée.

## PARTIE IV — Afrique : autonomie tertiaire et cloud régional (effets sur l’influence France/Europe)
### IV.1. LLM et « leapfrogging » : autonomie cognitive et services financiers

L’arrivée des grands modèles de langage modifie profondément les trajectoires de développement numérique des pays africains. Là où l’écart avec les économies européennes se mesurait historiquement en termes de capital humain spécialisé et d’accès à l’expertise, les LLM introduisent une dynamique de leapfrogging cognitif : ils permettent de sauter certaines étapes de maturation du tertiaire en internalisant rapidement des fonctions à forte intensité procédurale.

Dans le secteur bancaire, les effets sont particulièrement structurants. Des fonctions clés — KYC/AML, scoring de crédit, support client de premier niveau, back-office opérationnel, reporting réglementaire standard — deviennent automatisables ou semi-automatisables à coût marginal faible. Cette évolution rend possible l’internalisation de compétences qui étaient auparavant sous-traitées à des cabinets ou à des infrastructures européennes, souvent pour des raisons de complexité réglementaire ou de manque de ressources locales.

Il en résulte une réduction progressive de la dépendance à l’expertise européenne, non pas par rejet politique, mais par rationalité économique. Les banques africaines peuvent développer des systèmes plus adaptés aux réalités locales (données alternatives, usages mobiles, contextes informels) tout en améliorant leur efficacité opérationnelle. L’autonomie tertiaire cesse ainsi d’être un objectif lointain pour devenir une option crédible à moyen terme.

### **IV.2. Cloud africain : pas besoin d’hyperscale pour être souverain**

Cette autonomie cognitive trouve un prolongement naturel dans la question du cloud. Contrairement au modèle dominant porté par les hyperscalers, la souveraineté numérique africaine ne nécessite pas une infrastructure de calcul massive à l’échelle continentale. Une **stratégie réaliste** repose sur la constitution de clouds régionaux, adossés à des secteurs critiques (finance, télécoms, services publics), appuyés par des data centers souverains et interconnectés à l’échelle régionale.

Les LLM jouent ici un rôle d’accélérateur : ils réduisent les coûts d’exploitation, de maintenance et d’intégration des infrastructures cloud, permettant à des équipes plus restreintes d’opérer des systèmes complexes. La valeur ne réside pas dans la taille absolue du cloud, mais dans sa **capacité à répondre aux besoins locaux**, à garantir la disponibilité des données et à offrir un niveau de sécurité et de résilience suffisant.

Les acteurs susceptibles de porter ce modèle sont multiples : opérateurs télécoms disposant déjà des réseaux et des points de présence, banques cherchant à sécuriser leurs systèmes critiques, États soucieux de la souveraineté des données publiques, ou encore consortiums associant acteurs publics et privés. Cette pluralité favorise des architectures hybrides, moins dépendantes d’un acteur unique et plus résilientes face aux chocs externes.

### **IV.3. Implications géoéconomiques pour la France et l’Europe**

Pour la France et, plus largement, pour l’Europe, ces évolutions entraînent une **érosion progressive du soft power tertiaire**. L’influence historiquement exercée par le conseil, la conformité réglementaire, l’ingénierie financière ou les services IT tend à se réduire à mesure que ces fonctions s’automatisent et se localisent. Le risque est celui d’une perte silencieuse de positions, sans rupture visible mais par simple substitution technologique.

Cependant, cette transformation ouvre également une opportunité stratégique majeure. Plutôt que de chercher à préserver une rente de services en déclin, la France peut se repositionner comme **partenaire d’infrastructure**. Cela implique de privilégier des coopérations fondées sur le déploiement de clouds régionaux, l’interconnexion des réseaux, la formation des compétences locales et le renforcement de la cybersécurité.

Dans ce modèle, l’influence ne repose plus sur l’exportation de complexité ou de normes, mais sur la **co-construction d’infrastructures numériques durables**. Une telle approche permettrait à la France et à l’Europe de maintenir une présence stratégique en Afrique, non plus comme fournisseurs dominants de services tertiaires, mais comme acteurs structurants d’un écosystème numérique autonome et interopérable.

## **PARTIE V — Feuille de route France : doctrine, instruments et priorités d’exécution**

### **V.1. Définir une doctrine nationale de « numérique productif »**

La condition première d’une stratégie française crédible à l’ère des LLM est l’adoption d’une **doctrine claire de numérique productif**. L’enjeu n’est pas l’innovation pour elle-même, ni la course symbolique à la performance des modèles, mais la **transformation effective de l’IA en gains de productivité mesurables** dans l’industrie et les services critiques.

Cette doctrine repose sur un principe central : une **souveraineté pragmatique**. Il ne s’agit ni d’autarcie technologique, ni de rejet des écosystèmes globaux, mais du **contrôle des dépendances critiques**. La question structurante devient : dans quels domaines la dépendance est-elle acceptable, et dans lesquels constitue-t-elle un risque économique, juridique ou stratégique ? Les LLM rendent cette approche possible en réduisant le coût d’entrée pour des solutions nationales ou européennes, là où l’arbitrage était auparavant défavorable.

### **V.2. Trois piliers opérationnels pour l’exécution**

La doctrine doit se traduire par une action structurée autour de trois piliers opérationnels.

Le premier pilier concerne la **souveraineté des workloads critiques**. Les systèmes d’information de l’État, de la santé, de la finance, de la défense et de l’énergie doivent faire l’objet d’une politique explicite de localisation, de contrôle et d’audit. L’objectif n’est pas l’exclusivité nationale, mais la garantie que les fonctions vitales restent opérables, auditées et juridiquement maîtrisées en toute circonstance.

Le deuxième pilier porte sur l’**industrialisation du cloud de proximité**. La France ne peut ni ne doit chercher à reproduire le modèle des hyperscalers. En revanche, elle peut structurer un cloud hybride, réversible et certifié, capable de servir les besoins des administrations, des infrastructures critiques et des entreprises régulées. Les LLM permettent d’en réduire les coûts d’exploitation et d’en améliorer la qualité de service, rendant ce modèle économiquement soutenable.

Le troisième pilier est celui des **compétences et de l’outillage**. La valeur ne résidera plus dans la simple possession des modèles, mais dans la capacité à les intégrer, les opérer et les sécuriser. Cela implique un investissement massif dans les métiers de l’AIOps, du SecOps, de la gouvernance de l’IA, de la traçabilité des données (data lineage) et de l’architecture des systèmes complexes.

### **V.3. Instruments de politique économique**

La mise en œuvre de cette feuille de route nécessite des instruments économiques adaptés. Le premier est l’**achat public**, utilisé comme « client ancre » pour structurer un marché national et européen. En orientant la demande vers des solutions conformes aux exigences de souveraineté, l’État peut réduire le risque pour les acteurs privés et accélérer l’industrialisation.

Le second instrument est la **normalisation**. Des standards clairs en matière de réversibilité, d’auditabilité, de sécurité des systèmes d’IA et de gouvernance des données permettent de créer un avantage compétitif pour les acteurs capables de s’y conformer, tout en limitant les effets de verrouillage technologique.

À cela s’ajoutent des **soutiens ciblés au capital** : investissements dans les data centers et l’énergie associée, incitations à la localisation des données et des capacités de calcul, soutien aux infrastructures mutualisées. Enfin, des **sandboxes réglementaires**, notamment dans le secteur financier, peuvent permettre d’expérimenter des usages avancés de l’IA sous supervision, sans bloquer l’innovation par un excès de prudence ex ante.

### **V.4. Indicateurs de pilotage : mesurer pour gouverner**

Une politique du numérique productive ne peut être pilotée sans indicateurs clairs. Plusieurs **KPIs de niveau think tank** permettent d’évaluer la trajectoire réelle.

Le premier est la **part des workloads critiques hébergés en France ou dans l’Union européenne**, indicateur direct de la maîtrise opérationnelle. Le second est le **taux de réversibilité et de portabilité des environnements cloud**, mesuré objectivement par des tests de migration et d’audit.

S’y ajoutent des indicateurs de **productivité tertiaire**, tels que le coût moyen par dossier traité, par sinistre ou par contrôle réglementaire, permettant d’objectiver les gains liés à l’automatisation. Le **taux d’adoption de l’IA dans les ETI et l’industrie** constitue un autre signal clé de diffusion réelle de la technologie.

Enfin, la **dépendance aux GPU et la part de calcul local ou européen** doivent être suivies comme des indicateurs stratégiques à long terme, au même titre que l’énergie ou certaines matières premières critiques. Mesurer ces dépendances, c’est se donner les moyens de les arbitrer, plutôt que de les subir.

## Conclusion générale

L’analyse développée dans ce rapport met en évidence une transformation profonde et durable du paysage numérique. Les grands modèles de langage ne constituent pas une innovation marginale, mais un facteur de réorganisation structurelle de l’économie, en particulier dans les pays à forte tertiarisation comme la France. En abaissant le coût cognitif du développement, de l’intégration et de l’exploitation des systèmes complexes, ils déplacent les lignes de partage entre acteurs dominants et acteurs dépendants.

Cette dynamique affecte directement le tertiaire, longtemps fondé sur la rareté de l’expertise procédurale et sur l’intermédiation de la complexité. L’automatisation partielle de ces fonctions entraîne une compression des rentes existantes et une recomposition des chaînes de valeur. Elle ne signifie pas la disparition du tertiaire, mais sa transformation : la valeur se déplace vers les fonctions de conception, d’architecture, de gouvernance et de maîtrise des systèmes automatisés.

Dans ce contexte, le cloud apparaît comme le nouveau centre de gravité de l’autonomie numérique. À mesure que les LLM deviennent des briques industrielles, la localisation et le contrôle des workloads, des données et des capacités d’intégration deviennent déterminants. La baisse des coûts d’ingénierie rend désormais accessibles des modèles de cloud de proximité, hybrides et spécialisés, sans nécessiter l’atteinte d’une échelle hyperscale. L’autonomie ne se définit plus par la taille absolue des infrastructures, mais par leur adéquation aux usages critiques et leur capacité d’exploitation maîtrisée.

L’exemple africain éclaire ces évolutions sous un angle complémentaire. Les stratégies de leapfrogging rendues possibles par les LLM montrent que l’autonomie tertiaire et l’émergence de clouds régionaux peuvent s’opérer sans reproduire les trajectoires lourdes des économies avancées. Cette évolution modifie mécaniquement les rapports d’influence fondés sur l’exportation de services tertiaires et de complexité réglementaire, et invite à repenser les formes de coopération économique.

Enfin, la feuille de route esquissée pour la France souligne que l’enjeu central n’est pas l’adoption de l’IA en tant que telle, mais sa traduction en productivité réelle, en résilience et en capacité d’action. La combinaison d’une doctrine explicite, de priorités opérationnelles claires et d’indicateurs de pilotage permet d’aborder cette transformation comme un processus gouvernable, et non comme une évolution subie.

En définitive, les LLM ne suppriment ni les dépendances ni les asymétries, mais les reconfigurent. Ils déplacent la valeur des fonctions d’intermédiation vers celles de maîtrise des infrastructures et des usages, et redéfinissent les conditions dans lesquelles une économie peut conserver son autonomie dans un environnement numérique devenu plus accessible, mais aussi plus concurrentiel.
